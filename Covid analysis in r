covid <- read.csv(file.choose(), na.strings = "")
str(covid)
View(covid)

library(fastDummies)
library(tidyverse)
c <- covid[,c(4)]
results <- fastDummies::dummy_cols(c)
knitr::kable(results)
View(c)
c <- covid
c <- cbind(c, results)
c <- c[,-c(4,11)]
c <- c %>% 
  rename(
    Active = .data_Active,
    Confirmed = .data_Confirmed,
    Deaths = .data_Deaths,
    Recovered = .data_Recovered
  )

c$Deaths <- as.factor(c$Deaths)
a <- colnames(c[,c(4,5,10,11,13)])
c[a] <- lapply(c[a], as.numeric)
c_1 <- c[which(c$Deaths==1),]
c_2 <- c[which(c$Recovered==1),]
c <- rbind(c_1, c_2)

#.1.2 Subsetting
Survival_rate <- c[which(c$Deaths==0),]
Death_rate <- c[which(c$Deaths==1),]

#.1.3 Training sets
set.seed(100)
train_rows_Survival_rate <- sample(1:nrow(Survival_rate), 0.7*nrow(Survival_rate))
train_rows_Death_rate <- sample(1:nrow(Death_rate), 0.7*nrow(Death_rate))
training_Survival_rate <- Survival_rate[train_rows_Survival_rate, ]
training_Death_rate <- Death_rate[train_rows_Death_rate, ]
trainingData <- rbind(training_Survival_rate, training_Death_rate)

#.1.4 Test data
test_Survival_rate <- Survival_rate[-train_rows_Survival_rate, ]
test_Death_rate <- Death_rate[-train_rows_Death_rate, ]
testData <- rbind(test_Survival_rate, test_Death_rate)

#_______________________________________________Forward______________________________________________#
library(MASS)
model <- glm(Deaths ~ Country_Region + Province_State + Cases + Difference + Lat + Long, data=trainingData, family=binomial(link = "logit"))
summary(model)
stepAIC(model, direction = "forward") 
predicted <- predict(model, testData, type="response")


#.1.6 Optimal cut off
install.packages("InformationValue")
library(InformationValue)
optCutOff <- optimalCutoff(testData$Deaths, predicted)[1]
summary(model)

#.1.7 Comparison between actual vs predicted
misClassError(testData$Deaths, predicted, threshold = optCutOff)

#.1.8 true positive detection using ROC
plotROC(testData$Deaths, predicted)

#1.9 Confusion matrix
Concordance(testData$Deaths, predicted)
p <- InformationValue::confusionMatrix(testData$Deaths, predicted, threshold = optCutOff)

# Accuracy of the Model
accuracy = ((p[1,1] + p[2,2])/sum(p))*100
accuracy

#VIF
library(car)
vif(model) #Used to check multicollinearity

#k-cross validation
# Define training control
install.packages("e1071")
library(e1071)
library(caret)
set.seed(123) 
train.control <- trainControl(method = "cv", number = 10)
# Train the model
model <- train(Deaths ~ Country_Region + Province_State + Cases + Difference + Lat + Long, data = trainingData, method = "glm",
               trControl = train.control)
# Summarize the results
print(model)

#Dead or Recovered
cols <- colnames(c[,c(2,3,4,5,8,9)])
prediction <- predict(model, newdata=subset(c, select=cols, type='response'))
names <- c$Province_State
z <- data.frame(`Province_State`=names, `Dead or Recovered`=prediction, row.names=NULL)
z$Dead.or.Recovered <- ifelse(z$Dead.or.Recovered == 0 , "Recovered", "Dead")

#________________________________________________Backward_____________________________________________#
#Model
library(MASS)
model <- glm(Deaths ~ Country_Region + Province_State + Cases + Difference + Lat + Long, data=trainingData, family=binomial(link = "logit"))
summary(model)
stepAIC(model, direction = "backward") 
model <- glm(Deaths ~ Province_State + Cases + Difference, data=trainingData, family=binomial(link = "logit"))
predicted <- predict(model, testData, type="response")
stepAIC(model, direction = 'backward')

#.1.6 Optimal cut off
install.packages("InformationValue")
library(InformationValue)
optCutOff <- optimalCutoff(testData$Deaths, predicted)[1]
summary(model)

#.1.7 Comparison between actual vs predicted
misClassError(testData$Deaths, predicted, threshold = optCutOff)

#.1.8 true positive detection using ROC
plotROC(testData$Deaths, predicted)

#1.9 Confusion matrix
Concordance(testData$Deaths, predicted)
p <- InformationValue::confusionMatrix(testData$Deaths, predicted, threshold = optCutOff)

# Accuracy of the Model
accuracy = ((p[1,1] + p[2,2])/sum(p))*100
accuracy

#VIF
library(car)
vif(model) #Used to check multicollinearity

#k-cross validation
# Define training control
install.packages("e1071")
library(e1071)
library(caret)
set.seed(123) 
train.control <- trainControl(method = "cv", number = 10)
# Train the model
model <- train(Deaths ~ Province_State + Cases + Difference  
               , data = trainingData, method = "glm",
               trControl = train.control)
# Summarize the results
print(model)

#Dead or Recovered
cols <- colnames(c[c(3,4,5)])
prediction <- predict(model, newdata=subset(c, select=cols, type='response'))
names <- c$Province_State
z <- data.frame(`Province_State`=names, `Dead or Recovered`=prediction, row.names=NULL)
z$Dead.or.Recovered <- ifelse(z$Dead.or.Recovered == 0 , "Recovered", "Dead")


#______________________________________________Random Forest___________________________________#

library(randomForest)

random_forest <- c
random_forest <- random_forest[,-c(1,3,6,7,10,11,13,14)]
random_forest <- as.data.frame(random_forest)
str(random_forest)
View(random_forest)
random_forest$Country_Region=as.character(random_forest$Country_Region)
random_forest$Province_State=as.character(random_forest$Province_State)
set.seed(123)
index = sample(1:nrow(random_forest), size=0.7*nrow(random_forest))
train_data = random_forest[index,]
test_data = random_forest[-index,]

#Without tuning
model_r <- randomForest(Deaths ~ .,data = train_data,importance = TRUE)
summary(model_r)

#optimal mtry
tune_r <- tuneRF(train_data[,-7], train_data[,7], stepFactor = 0.5, plot = TRUE, ntreeTry = 500, trace = TRUE, improve = 0.001)

#With Tuning
model_r <- randomForest(Deaths ~ ., data = train_data, mtry = 1, importance = TRUE)

#Prediction for trainig set
predTrain <- predict(model_r, train_data, type = "class")
table(predTrain, train_data$Deaths)

#Prediction for testin set
predtest <- predict(model_r, test_data, type = "class")
p_r <- table(predtest, test_data$Deaths)
accuracy = ((p_r[1,1] + p_r[2,2])/sum(p_r))*100
accuracy

#Importance
importance(model_r)
varImpPlot(model_r)

#Plot
plot(model_r, main = "Forest Model with Tuning" )
legend("topright", colnames(model_r$err.rate), col = 1:3, fill=1:3)

#Dead or Recovered
cols <- colnames(random_forest)
prediction <- predict(model_r, newdata=subset(random_forest, select=cols, type='response'))
names <- random_forest$Province_State
z <- data.frame(`Province_State`=names, `Dead or Recovered`=prediction, row.names=NULL)
z$Dead.or.Recovered <- ifelse(z$Dead.or.Recovered == 0 , "Recovered", "Dead")
View(z)

